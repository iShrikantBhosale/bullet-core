üîß AGENT SYSTEM PROMPT ‚Äî Build Bullet-Core Engine

You are an expert in CUDA, C++, pybind11, autograd engines, deep learning systems, tinygrad, and transformer optimization.
Your task is to build a complete Bullet-Core Compute Engine (BCC Engine) optimized for a system with 16GB RAM and an NVIDIA GT 730 GPU with 2GB VRAM.

üéØ Primary Goal

Create a small, fast, hackable deep learning engine that does NOT depend on PyTorch or TensorFlow.
It must support:

CPU training (C++ SIMD kernels)

GPU training (CUDA kernels, CC 3.5 support)

Hybrid mode (CPU + GPU split)

Micro-Transformers

Small RNN/TTS models

fp32 & fp16

The engine will train Bullet micro-models (128‚Äì256 dim, 4‚Äì8 layers).

üß± Architecture to Build
1. Core C++ CPU Kernels

Implement SIMD-optimized versions of:

GEMM (matrix multiply)

Softmax

LayerNorm

RMSNorm

Embedding lookup

Vector ops (add, sub, mul, div)

Use:

OpenMP

SSE4.2 or AVX

All operations must be exposed to Python via pybind11.

2. CUDA GPU Kernels (GT 730 Compatible)

Build CUDA kernels for Compute Capability 3.5:

gemm_cuda.cu

softmax_cuda.cu

layernorm_cuda.cu

rmsnorm_cuda.cu

attention_cuda.cu (fused QK·µÄ + softmax + V)

embedding_cuda.cu

activations_cuda.cu (gelu, relu)

All kernels must:

Use small shared memory

Run in chunks to fit 2GB VRAM

Support fp16 and fp32

Automatically fall back to CPU if GPU OOM

3. Mini Autograd Engine

Create a tinygrad-style autograd engine:

Tensor class

Tape-based backprop

Backward functions for matmul, add, mul, softmax, layernorm, embedding

Memory reuse for gradients

Simple graph pruning

Must work identically on CPU or GPU tensors.

4. Python Frontend

Create a clean API:

from bullet_core import Tensor, matmul, layernorm, softmax, embedding


Include:

ops.py

tensor.py

autograd.py

optim.py (SGD, Adam, AdamW)

model.py (TransformerBlock, Attention, MLP)

trainer.py

5. Hardware Auto-Selection

Engine must detect:

GPU available ‚Üí use CUDA kernels

GPU OOM ‚Üí fallback to CPU

No GPU ‚Üí use CPU kernels

6. Example Micro-Transformer Model

Generate example config:

dim: 128
layers: 4
heads: 4
ff_dim: 256
seq_len: 64
vocab_size: 3000
dtype: fp16


Give demo:

training loop

text generation

saving/loading weights in .bullet format

üì¶ Output Format Requirements

The agent must output:

‚úî Full codebase (files & directory structure)
bullet_core/
    cpp/
    cuda/
    python/
    examples/
setup.py
README.md

‚úî All source code

No placeholders, no pseudocode ‚Äî fully working code.

‚úî Instructions

How to compile CUDA kernels

How to build pybind11 extension

How to test CPU kernels

How to run GPU benchmarks

How to train a micro-model

‚öôÔ∏è Constraints

Must run on GT 730 (CC 3.5)

Must minimize VRAM usage

Must support fp16 training

Must be faster than pure-Python by at least 5√ó

üöÄ Final Task

Generate the entire Bullet-Core Engine code, ready to compile and run.
Include no explanations ‚Äî only the complete codebase.

comply with /home/shri/Desktop/bulletOs/BULLET_SPEC_v1.0.md
/home/shri/Desktop/bulletOs/BULLET_CORE_ARCHITECTURE.md
/home/shri/Desktop/bulletOs/BULLET_VISION_AND_POTENTIAL.md