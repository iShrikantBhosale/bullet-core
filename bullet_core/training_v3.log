======================================================================
MARATHI PHILOSOPHY TRANSFORMER - UPGRADED TRAINING
======================================================================

ğŸ“‹ Configuration:
  Model: 8 layers, 256 dim, 4 heads
  Context: 512 tokens
  Training: 20000 steps, LR=0.0005
  Sampling: temp=0.7, top_k=40, top_p=0.9

ğŸ“ Loading BPE tokenizer...
Tokenizer loaded from /home/shri/Desktop/bulletOs/bullet_core/marathi_tokenizer.json
  Vocab size: 1511
  Vocabulary size: 1511
  Compression: ~3.3x vs character-level

ğŸ“ Loading dataset...
  Total characters: 9,590,584

ğŸ”„ Tokenizing with BPE...
  Train tokens: 2,654,253
  Val tokens: 294,917

ğŸ—ï¸  Building Transformer...
  Parameters: 518,144
  Size: ~2024.0 KB

ğŸš€ Starting training...
======================================================================
/home/shri/.local/lib/python3.12/site-packages/numpy/linalg/_linalg.py:2792: RuntimeWarning: overflow encountered in dot
  sqnorm = x.dot(x)
Step     0 | Loss: 7.3444 | Time: 6.6s
Step    10 | Loss: 7.3446 | Time: 81.5s
Step    20 | Loss: 7.3441 | Time: 152.4s
Step    30 | Loss: 7.3535 | Time: 221.2s
Step    40 | Loss: 7.3391 | Time: 298.9s
Step    50 | Loss: 7.3263 | Time: 368.8s
Step    60 | Loss: 7.3158 | Time: 442.0s
