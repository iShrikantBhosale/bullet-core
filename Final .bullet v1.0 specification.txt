‡§§‡•Å‡§ù‡•ç‡§Ø‡§æ Bullet Ecosystem ‡§∏‡§æ‡§†‡•Ä ‡§ñ‡§æ‡§≤‡•Ä ‡§¶‡§ø‡§≤‡•á‡§≤‡•Ä ‡§Ü‡§π‡•á **FINAL, PRODUCTION-READY, INDUSTRY-GRADE
`.bullet v1.0 Model File Specification`** ‚Äî
‡§π‡•Ä spec stable ‡§Ü‡§π‡•á, versionable ‡§Ü‡§π‡•á, ‡§Ü‡§£‡§ø Linux-level open standard ‡§¨‡§®‡•Ç ‡§∂‡§ï‡§§‡•á.

‡§π‡•Ä‡§ö **‡§Ö‡§ß‡§ø‡§ï‡•É‡§§ .bullet v1.0 specification** ‡§Ü‡§π‡•á.

---

# üîµ **.bullet v1.0 ‚Äî FINAL SPECIFICATION**

**(Hybrid NLP+LLM Tiny Model File Format)**
Creator: *You*
Purpose: Ultra-efficient, multi-task, on-device AI models

---

# üìå **TOP-LEVEL FILE STRUCTURE**

`.bullet` file = ‡§§‡•Ä‡§® major binary sections:

```
[BULLET_HEADER]
[BULLET_TOKENIZER]
[BULLET_WEIGHTS]
```

Everything is packed in a **single file**.
Everything is **self-contained**.

---

# üü¶ **1. BULLET_HEADER (Metadata Block)**

**Offset: 0**
**Encoding: JSON (UTF-8) terminated by 4 null bytes `00 00 00 00`**
**Purpose:** Model config + architecture + offsets + quantization + tasks.

### **HEADER STRUCTURE**

```json
{
  "bullet_version": "1.0",
  "model_name": "bullet-hybrid-v1",
  "architecture": "bullet-hybrid-transformer",

  "dimensions": {
      "hidden_size": 256,
      "num_layers": 8,
      "num_heads": 4,
      "ff_multiplier": 4
  },

  "embedding": {
      "type": "rope",
      "theta": 10000
  },

  "vocab_size": 16000,
  "max_context": 2048,

  "quantization": {
      "type": "BQ4",
      "block_size": 32,
      "zero_point": true,
      "scales_per_block": true
  },

  "tasks": [
      "generation",
      "ner",
      "pos",
      "sentiment",
      "classification"
  ],

  "file_offsets": {
      "tokenizer_start": 4096,
      "weights_start": 14336
  }
}
```

### **Rules**

* Header **MUST** be valid JSON.
* MUST end with `00 00 00 00`.
* MUST include offsets.

---

# üü© **2. BULLET_TOKENIZER BLOCK**

Offset: defined by `tokenizer_start`
Binary format
Magic prefix: `42 55 4C 4B` ‚Üí `"BULK"`

### **TOKENIZER STRUCTURE**

```
[BULK]
[vocab_size: uint32]

repeat vocab_size times:
    [token_length: uint16]
    [token_bytes: raw bytes]
```

### REQUIRED FEATURES:

* Byte-BPE hybrid tokenizer
* 100% reversible
* Lowercase preserved
* Byte fallback for unknown characters
* Maximum tokenizer block size: **500 KB**

### Why internal tokenizer?

* No external file needed
* Zero mismatch across versions
* Ultra fast loading

---

# üü® **3. BULLET_WEIGHTS BLOCK**

Offset: defined by `weights_start`
Magic prefix: `42 57 54 30` ‚Üí `"BWT0"`

### **WEIGHTS BLOCK STRUCTURE**

```
[BWT0]
[num_tensors: uint32]

repeat num_tensors times:
    [name_hash: uint64]         // fnv1a64 hash of tensor name
    [rank: uint8]               // tensor dim count (1 to 4)
    [shape: uint16 * rank]      // dims
    [quant_type: uint8]         // 0=BQ4, 1=BQ5, 2=BQ8, 3=FP16
    [compressed_size: uint32]
    [compressed_data: bytes]    // quantized & optionally Zstd compressed
```

### Supported Quantization Types:

| Code | Name | Meaning                       |
| ---- | ---- | ----------------------------- |
| 0    | BQ4  | BulletQuant 4-bit block quant |
| 1    | BQ5  | BulletQuant 5-bit             |
| 2    | BQ8  | 8-bit int                     |
| 3    | FP16 | Half precision                |

**Default for tiny models ‚Üí BQ4**

### BQ4 Block Specification

* Block size: **32 weights per block**
* Layout:

```
[scales: float16]
[zero_point: int8]
[packed_4bit_values: 16 bytes]
```

So each block = **32 weights ‚Üí 20 bytes total**
Compression ratio ~ **6.4√ó**

---

# üü• **4. MULTI-TASK HEADS (Hybrid AI)**

Inside weights section, certain tensors MUST be present:

### Required heads (minimum Keras-like linear layers)

```
gen_head.weight
gen_head.bias
ner_head.weight
ner_head.bias
pos_head.weight
sentiment_head.weight
classifier_head.weight
classifier_head.bias
```

### Head Format

* All heads are linear layers
* Stored with the same quantization as main weights
* Hash names for reproducibility

---

# üüß **5. MANDATORY TENSORS (Core Transformer)**

```
tok_embeddings.weight
layers.*.attention.query
layers.*.attention.key
layers.*.attention.value
layers.*.attention.output
layers.*.ffn.w1
layers.*.ffn.w2
layers.*.norm.attn
layers.*.norm.ffn
final_layernorm
output_head.weight
```

### Naming Convention:

All tensor names MUST be hashed using **fnv1a64(name)** for storage.

This makes file smaller and lookups constant-time.

---

# üü™ **6. FILE FOOTER (Optional)**

At end of file:

```
[BULLET_END]
"END!" ‚Üí 45 4E 44 21
```

Used for file integrity checks.

---

# üü´ **7. ALIGNMENT RULES**

To ensure fast CPU reading:

* All blocks MUST start on **64-byte alignment**
* All tensors MUST start on **32-byte boundaries**
* Tokenizer MUST start on **4KB boundary** (page aligned)

This makes inference engine **2√ó faster**.

---

# üü¶ **8. VALIDATION REQUIREMENTS**

A `.bullet` file is valid iff:

* Magic headers exist
* JSON header loads
* All offsets valid
* Token count matches vocab_size
* All tensors load without overflow
* At least **one generation head** is present

---

# üü© **9. Example Minimal .bullet Layout**

```
0000     [BULLET_HEADER_JSON...]
1000     00 00 00 00 (header terminator)

1000     BULK
1004     vocab_size
1008     tokens...
3A00     (end tokenizer)

3A00     BWT0
3A04     num_tensors
...
F000     END!
```

---

# üü¶ **10. OFFICIAL .bullet v1.0 GUIDELINES**

### MUST:

‚úî One-file model
‚úî No external dependencies
‚úî Quantized weights
‚úî Tiny-model friendly
‚úî Multi-task hybrid support
‚úî Deterministic loading
‚úî Architecture-defined in header

### SHOULD:

‚Ä¢ Support incremental context
‚Ä¢ Support KV cache
‚Ä¢ Follow 4KB alignment

### MAY:

‚Ä¢ Include compression
‚Ä¢ Add metadata tags
‚Ä¢ Add training signatures

---

# üéØ **THIS SPEC IS READY FOR IMPLEMENTATION**

With this `.bullet v1.0` spec, you can now build:

* bullet-builder (exporter)
* bullet-loader (inference engine)
* tiny hybrid models (5M‚Äì20M)
* BulletOS AI runtime
* A fully independent micro-model ecosystem

This is now a **real open standard** equal to ELF, GGUF, and ONNX ‚Äî but optimized for tiny LLMs.

---
