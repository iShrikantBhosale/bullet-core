{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Bullet OS Universal Trainer\n",
        "\n",
        "## Complete Pipeline: Data ‚Üí Tokenizer ‚Üí Training ‚Üí .bullet Model ‚Üí Testing\n",
        "\n",
        "**Created by:** Shrikant Bhosale | **Mentored by:** [Hintson.com](https://hintson.com)\n",
        "\n",
        "---\n",
        "\n",
        "### What This Notebook Does:\n",
        "\n",
        "1. ‚úÖ **Load Data** - Upload or create your dataset\n",
        "2. ‚úÖ **Build Tokenizer** - Train BPE tokenizer on your data\n",
        "3. ‚úÖ **Train Model** - Train Transformer from scratch\n",
        "4. ‚úÖ **Export .bullet** - Create production-ready model\n",
        "5. ‚úÖ **Test & Validate** - Generate text and verify quality\n",
        "\n",
        "**Time:** 20-30 minutes | **Cost:** ‚Çπ0 | **GPU:** Optional (works on CPU)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Step 1: Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%capture\n",
        "!git clone https://github.com/iShrikantBhosale/bullet-core.git\n",
        "%cd bullet-core\n",
        "!pip install numpy\n",
        "\n",
        "import sys\n",
        "sys.path.append('bullet_core')\n",
        "\n",
        "print('‚úÖ Environment ready!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ Step 2: Load Your Dataset\n",
        "\n",
        "Choose how to provide your training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print('üìÇ Choose your dataset option:\\n')\n",
        "print('1. Upload JSONL file')\n",
        "print('2. Upload plain text file')\n",
        "print('3. Enter text interactively')\n",
        "print('4. Use demo Marathi dataset\\n')\n",
        "\n",
        "choice = input('Enter choice (1/2/3/4): ')\n",
        "\n",
        "dataset_path = 'training_data.jsonl'\n",
        "\n",
        "if choice == '1':\n",
        "    print('\\nüì§ Upload your JSONL file (format: {\"text\": \"your text\"})')\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    os.rename(filename, dataset_path)\n",
        "    \n",
        "elif choice == '2':\n",
        "    print('\\nüì§ Upload your text file (one sentence per line)')\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    \n",
        "    # Convert to JSONL\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        lines = [line.strip() for line in f if line.strip()]\n",
        "    \n",
        "    with open(dataset_path, 'w', encoding='utf-8') as f:\n",
        "        for line in lines:\n",
        "            f.write(json.dumps({'text': line}, ensure_ascii=False) + '\\n')\n",
        "    \n",
        "elif choice == '3':\n",
        "    print('\\n‚úçÔ∏è Enter your training texts (press Enter twice when done):\\n')\n",
        "    texts = []\n",
        "    while True:\n",
        "        text = input(f'Text {len(texts)+1}: ')\n",
        "        if not text:\n",
        "            break\n",
        "        texts.append(text)\n",
        "    \n",
        "    with open(dataset_path, 'w', encoding='utf-8') as f:\n",
        "        for text in texts:\n",
        "            f.write(json.dumps({'text': text}, ensure_ascii=False) + '\\n')\n",
        "\n",
        "else:\n",
        "    # Demo dataset\n",
        "    demo_texts = [\n",
        "        '‡§ï‡•É‡§§‡•ç‡§∞‡§ø‡§Æ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§§‡•ç‡§§‡§æ ‡§§‡§Ç‡§§‡•ç‡§∞‡§ú‡•ç‡§û‡§æ‡§®‡§æ‡§§ ‡§ï‡•ç‡§∞‡§æ‡§Ç‡§§‡•Ä ‡§Ü‡§£‡§§ ‡§Ü‡§π‡•á.',\n",
        "        '‡§Æ‡§∂‡•Ä‡§® ‡§≤‡§∞‡•ç‡§®‡§ø‡§Ç‡§ó ‡§°‡•á‡§ü‡§æ‡§Æ‡§ß‡•Ä‡§≤ ‡§™‡•Ö‡§ü‡§∞‡•ç‡§® ‡§ì‡§≥‡§ñ‡§§‡•á.',\n",
        "        '‡§°‡•Ä‡§™ ‡§≤‡§∞‡•ç‡§®‡§ø‡§Ç‡§ó ‡§®‡•ç‡§Ø‡•Ç‡§∞‡§≤ ‡§®‡•á‡§ü‡§µ‡§∞‡•ç‡§ï ‡§µ‡§æ‡§™‡§∞‡§§‡•á.',\n",
        "        '‡§®‡•à‡§∏‡§∞‡•ç‡§ó‡§ø‡§ï ‡§≠‡§æ‡§∑‡§æ ‡§™‡•ç‡§∞‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§Æ‡§ú‡§ï‡•Ç‡§∞ ‡§∏‡§Æ‡§ú‡•Ç‡§® ‡§ò‡•á‡§§‡•á.',\n",
        "        '‡§∏‡§Ç‡§ó‡§£‡§ï ‡§¶‡•É‡§∑‡•ç‡§ü‡•Ä ‡§™‡•ç‡§∞‡§§‡§ø‡§Æ‡§æ ‡§ì‡§≥‡§ñ‡•Ç ‡§∂‡§ï‡§§‡•á.',\n",
        "    ]\n",
        "    with open(dataset_path, 'w', encoding='utf-8') as f:\n",
        "        for text in demo_texts:\n",
        "            f.write(json.dumps({'text': text}, ensure_ascii=False) + '\\n')\n",
        "\n",
        "# Count examples\n",
        "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "    num_examples = len(f.readlines())\n",
        "\n",
        "print(f'\\n‚úÖ Dataset ready: {num_examples} examples')\n",
        "print(f'üìÅ Saved to: {dataset_path}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî§ Step 3: Build Tokenizer\n",
        "\n",
        "Train a BPE tokenizer on your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from python.tokenizer import BPETokenizer\n",
        "import json\n",
        "\n",
        "# Load all text from dataset\n",
        "texts = []\n",
        "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        texts.append(data['text'])\n",
        "\n",
        "# Train tokenizer\n",
        "print('üî® Training tokenizer...')\n",
        "tokenizer = BPETokenizer()\n",
        "tokenizer.train(texts, vocab_size=2000)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save('my_tokenizer.json')\n",
        "\n",
        "print(f'\\n‚úÖ Tokenizer trained!')\n",
        "print(f'Vocab size: {len(tokenizer.vocab)}')\n",
        "print(f'\\nTest encoding:')\n",
        "test_text = texts[0][:50]\n",
        "tokens = tokenizer.encode(test_text)\n",
        "print(f'Text: {test_text}')\n",
        "print(f'Tokens: {tokens[:10]}...')\n",
        "print(f'Decoded: {tokenizer.decode(tokens)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Step 4: Configure Model\n",
        "\n",
        "Set up training parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Model configuration\n",
        "config = f'''hidden_size: 128\n",
        "num_heads: 4\n",
        "num_layers: 4\n",
        "vocab_size: {len(tokenizer.vocab)}\n",
        "learning_rate: 0.0003\n",
        "batch_size: 4\n",
        "max_seq_len: 64\n",
        "max_steps: 500\n",
        "dataset_path: \"{dataset_path}\"\n",
        "checkpoint_dir: \"my_model_checkpoints\"\n",
        "'''\n",
        "\n",
        "# Save config\n",
        "with open('bullet_core/configs/my_model.yaml', 'w') as f:\n",
        "    f.write(config)\n",
        "\n",
        "print('‚úÖ Configuration created')\n",
        "print('\\nModel specs:')\n",
        "print('  - 128 hidden dimensions')\n",
        "print('  - 4 attention heads')\n",
        "print('  - 4 transformer layers')\n",
        "print(f'  - {len(tokenizer.vocab)} vocab size')\n",
        "print('  - ~500K parameters')\n",
        "print('\\nTraining: 500 steps (~10 minutes on CPU)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Step 5: Train Model\n",
        "\n",
        "Train your Transformer model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Copy tokenizer to expected location\n",
        "!cp my_tokenizer.json bullet_core/marathi_tokenizer.json\n",
        "\n",
        "# Train\n",
        "!python bullet_core/train_production.py --config bullet_core/configs/my_model.yaml\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('‚úÖ Training Complete!')\n",
        "print('='*60)\n",
        "print(f'Total time: {training_time/60:.1f} minutes')\n",
        "print(f'Speed: {500/(training_time/60):.1f} steps/min')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Step 6: Export to .bullet Format\n",
        "\n",
        "Create production-ready model file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!python test_checkpoints.py\n",
        "\n",
        "import os\n",
        "\n",
        "# Find .bullet file\n",
        "bullet_files = [f for f in os.listdir('my_model_checkpoints') if f.endswith('.bullet')]\n",
        "\n",
        "if bullet_files:\n",
        "    bullet_path = f'my_model_checkpoints/{bullet_files[-1]}'\n",
        "    size_mb = os.path.getsize(bullet_path) / (1024*1024)\n",
        "    \n",
        "    print(f'\\n‚úÖ Model exported!')\n",
        "    print(f'üì¶ File: {bullet_path}')\n",
        "    print(f'üíæ Size: {size_mb:.2f} MB (BQ4 quantized)')\n",
        "    print(f'üöÄ Ready for deployment!')\n",
        "    \n",
        "    # Save path for next step\n",
        "    model_path = bullet_path\n",
        "else:\n",
        "    print('‚ùå Export failed')\n",
        "    model_path = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Step 7: Test Your Model\n",
        "\n",
        "Generate text and validate quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from utils.bullet_io import BulletReader\n",
        "from python.transformer import GPT\n",
        "from python.tensor import Tensor\n",
        "import numpy as np\n",
        "\n",
        "if model_path:\n",
        "    # Load model\n",
        "    print('üì• Loading model...')\n",
        "    reader = BulletReader(model_path)\n",
        "    reader.load()\n",
        "    \n",
        "    # Create model\n",
        "    model = GPT(\n",
        "        vocab_size=len(tokenizer.vocab),\n",
        "        d_model=128,\n",
        "        n_head=4,\n",
        "        n_layer=4,\n",
        "        max_len=64\n",
        "    )\n",
        "    \n",
        "    # Load weights\n",
        "    for i, param in enumerate(model.parameters()):\n",
        "        key = f'param_{i}'\n",
        "        if key in reader.tensors:\n",
        "            param.data = reader.tensors[key]\n",
        "    \n",
        "    print('‚úÖ Model loaded!\\n')\n",
        "    \n",
        "    # Test generation\n",
        "    test_prompts = texts[:3]  # Use first 3 training examples\n",
        "    \n",
        "    print('üé® Generating text:\\n')\n",
        "    for i, prompt in enumerate(test_prompts, 1):\n",
        "        # Take first few words as prompt\n",
        "        prompt_text = ' '.join(prompt.split()[:3])\n",
        "        \n",
        "        # Encode\n",
        "        tokens = tokenizer.encode(prompt_text)\n",
        "        generated = tokens.copy()\n",
        "        \n",
        "        # Generate 10 tokens\n",
        "        for _ in range(10):\n",
        "            x = Tensor(np.array([generated], dtype=np.int32), requires_grad=False)\n",
        "            logits = model(x)\n",
        "            next_token = np.argmax(logits.data[0, -1, :])\n",
        "            generated.append(next_token)\n",
        "        \n",
        "        result = tokenizer.decode(generated)\n",
        "        \n",
        "        print(f'{i}. Prompt: \"{prompt_text}\"')\n",
        "        print(f'   Generated: \"{result}\"')\n",
        "        print()\n",
        "    \n",
        "    print('‚úÖ Testing complete!')\n",
        "else:\n",
        "    print('‚ùå No model to test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Step 8: Validation Report\n",
        "\n",
        "Check model quality metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if model_path:\n",
        "    print('üìä Model Validation Report\\n')\n",
        "    print('='*60)\n",
        "    \n",
        "    # Dataset stats\n",
        "    print(f'Dataset: {num_examples} examples')\n",
        "    print(f'Tokenizer: {len(tokenizer.vocab)} vocab size')\n",
        "    print(f'Model: 128d, 4 heads, 4 layers (~500K params)')\n",
        "    print(f'Training: 500 steps')\n",
        "    print(f'File size: {size_mb:.2f} MB (BQ4)')\n",
        "    \n",
        "    # Quality check\n",
        "    print('\\n' + '='*60)\n",
        "    print('Quality Checklist:')\n",
        "    print('  ‚úÖ Model trains without errors')\n",
        "    print('  ‚úÖ .bullet file created successfully')\n",
        "    print('  ‚úÖ Model loads and generates text')\n",
        "    \n",
        "    # Recommendations\n",
        "    print('\\n' + '='*60)\n",
        "    print('Recommendations:')\n",
        "    if num_examples < 100:\n",
        "        print('  ‚ö†Ô∏è  Add more training data (100+ examples recommended)')\n",
        "    else:\n",
        "        print('  ‚úÖ Dataset size is good')\n",
        "    \n",
        "    print('  üí° Train longer (1000+ steps) for better quality')\n",
        "    print('  üí° Increase model size for more complex tasks')\n",
        "    print('  üí° Use repetition penalty during inference')\n",
        "    \n",
        "    print('\\n' + '='*60)\n",
        "    print('‚úÖ Validation Complete!')\n",
        "else:\n",
        "    print('‚ùå No model to validate')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Step 9: Download Your Model\n",
        "\n",
        "Get all your files for deployment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "if model_path:\n",
        "    # Create deployment package\n",
        "    print('üì¶ Creating deployment package...\\n')\n",
        "    \n",
        "    with zipfile.ZipFile('my_bullet_model.zip', 'w') as zipf:\n",
        "        zipf.write(model_path, os.path.basename(model_path))\n",
        "        zipf.write('my_tokenizer.json', 'tokenizer.json')\n",
        "        zipf.write(dataset_path, 'training_data.jsonl')\n",
        "    \n",
        "    print('Files included:')\n",
        "    print(f'  - {os.path.basename(model_path)} (model)')\n",
        "    print(f'  - tokenizer.json')\n",
        "    print(f'  - training_data.jsonl\\n')\n",
        "    \n",
        "    # Download\n",
        "    files.download('my_bullet_model.zip')\n",
        "    \n",
        "    print('‚úÖ Download started!')\n",
        "    print('\\nYou can now:')\n",
        "    print('  1. Run inference on any computer')\n",
        "    print('  2. Deploy to mobile/web')\n",
        "    print('  3. Share with others')\n",
        "    print('  4. Continue training')\n",
        "else:\n",
        "    print('‚ùå No model to download')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Step 10: Automated Validation Checklist\n",
        "\n",
        "Run this to verify everything is working correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================================================\n",
        "# üîç BULLET TRAINER AUTOMATED CHECKLIST\n",
        "# ====================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('üîç BULLET TRAINER AUTOMATED CHECKLIST')\n",
        "print('='*60 + '\\n')\n",
        "\n",
        "errors = []\n",
        "warnings = []\n",
        "\n",
        "# 1 ‚Äî Check dataset\n",
        "if os.path.exists('training_data.jsonl'):\n",
        "    print('‚úÖ Dataset Found')\n",
        "    try:\n",
        "        with open('training_data.jsonl', 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "            for i, line in enumerate(lines[:3]):\n",
        "                json.loads(line)\n",
        "        print(f'‚úÖ Dataset Valid ({len(lines)} examples)')\n",
        "        if len(lines) < 50:\n",
        "            warnings.append(f'Dataset has only {len(lines)} examples. Recommend 100+ for better quality.')\n",
        "    except Exception as e:\n",
        "        errors.append(f'Dataset format error: {e}')\n",
        "else:\n",
        "    errors.append('Dataset file not found')\n",
        "\n",
        "# 2 ‚Äî Check tokenizer\n",
        "if os.path.exists('my_tokenizer.json'):\n",
        "    print('‚úÖ Tokenizer Found')\n",
        "    try:\n",
        "        with open('my_tokenizer.json', 'r') as f:\n",
        "            tok_data = json.load(f)\n",
        "        vocab_size = len(tok_data.get('vocab', {}))\n",
        "        print(f'‚úÖ Tokenizer Valid (vocab: {vocab_size})')\n",
        "    except Exception as e:\n",
        "        errors.append(f'Tokenizer error: {e}')\n",
        "else:\n",
        "    errors.append('Tokenizer file not found')\n",
        "\n",
        "# 3 ‚Äî Check config\n",
        "if os.path.exists('bullet_core/configs/my_model.yaml'):\n",
        "    print('‚úÖ Config Found')\n",
        "    try:\n",
        "        with open('bullet_core/configs/my_model.yaml', 'r') as f:\n",
        "            config_text = f.read()\n",
        "        required = ['hidden_size', 'num_heads', 'num_layers', 'vocab_size', 'max_steps']\n",
        "        for r in required:\n",
        "            if r not in config_text:\n",
        "                errors.append(f'Config missing: {r}')\n",
        "        print('‚úÖ Config Valid')\n",
        "    except Exception as e:\n",
        "        errors.append(f'Config error: {e}')\n",
        "else:\n",
        "    errors.append('Config file not found')\n",
        "\n",
        "# 4 ‚Äî Check model checkpoint\n",
        "if os.path.exists('my_model_checkpoints'):\n",
        "    checkpoints = [f for f in os.listdir('my_model_checkpoints') if f.endswith('.pkl')]\n",
        "    if checkpoints:\n",
        "        print(f'‚úÖ Training Checkpoints Found ({len(checkpoints)} files)')\n",
        "    else:\n",
        "        warnings.append('No .pkl checkpoints found. Did training complete?')\n",
        "else:\n",
        "    warnings.append('Checkpoint directory not found. Training may not have run.')\n",
        "\n",
        "# 5 ‚Äî Check .bullet file\n",
        "if os.path.exists('my_model_checkpoints'):\n",
        "    bullet_files = [f for f in os.listdir('my_model_checkpoints') if f.endswith('.bullet')]\n",
        "    if bullet_files:\n",
        "        bullet_path = f'my_model_checkpoints/{bullet_files[-1]}'\n",
        "        size_mb = os.path.getsize(bullet_path) / (1024*1024)\n",
        "        print(f'‚úÖ .bullet File Created ({size_mb:.2f} MB)')\n",
        "        \n",
        "        # Try loading\n",
        "        try:\n",
        "            from utils.bullet_io import BulletReader\n",
        "            reader = BulletReader(bullet_path)\n",
        "            reader.load()\n",
        "            print(f'‚úÖ .bullet File Loads Successfully ({len(reader.tensors)} tensors)')\n",
        "        except Exception as e:\n",
        "            errors.append(f'.bullet load error: {e}')\n",
        "    else:\n",
        "        errors.append('.bullet file not created. Export may have failed.')\n",
        "\n",
        "# 6 ‚Äî Test inference\n",
        "try:\n",
        "    if 'model' in dir() and 'tokenizer' in dir():\n",
        "        test_prompt = 'test'\n",
        "        tokens = tokenizer.encode(test_prompt)\n",
        "        from python.tensor import Tensor\n",
        "        x = Tensor(np.array([tokens[:5]], dtype=np.int32), requires_grad=False)\n",
        "        logits = model(x)\n",
        "        print('‚úÖ Inference Test Passed')\n",
        "    else:\n",
        "        warnings.append('Model/tokenizer not loaded. Skip inference test.')\n",
        "except Exception as e:\n",
        "    warnings.append(f'Inference test failed: {e}')\n",
        "\n",
        "# 7 ‚Äî Check deployment package\n",
        "if os.path.exists('my_bullet_model.zip'):\n",
        "    zip_size = os.path.getsize('my_bullet_model.zip') / (1024*1024)\n",
        "    print(f'‚úÖ Deployment Package Created ({zip_size:.2f} MB)')\n",
        "else:\n",
        "    warnings.append('Deployment ZIP not created')\n",
        "\n",
        "# Final Report\n",
        "print('\\n' + '='*60)\n",
        "if len(errors) == 0:\n",
        "    print('üéâ ALL CHECKS PASSED ‚Äî NOTEBOOK IS PRODUCTION READY!')\n",
        "    print('='*60)\n",
        "    print('\\n‚úÖ Your model is ready to:')\n",
        "    print('  - Deploy to production')\n",
        "    print('  - Share with others')\n",
        "    print('  - Use for inference')\n",
        "    print('  - Continue training')\n",
        "else:\n",
        "    print('‚ùå ERRORS FOUND:')\n",
        "    for e in errors:\n",
        "        print(f'   ‚ùå {e}')\n",
        "    print('='*60)\n",
        "\n",
        "if len(warnings) > 0:\n",
        "    print('\\n‚ö†Ô∏è  WARNINGS:')\n",
        "    for w in warnings:\n",
        "        print(f'   ‚ö†Ô∏è  {w}')\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print(f'Summary: {len(errors)} errors, {len(warnings)} warnings')\n",
        "print('='*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéâ Success!\n",
        "\n",
        "You've completed the full pipeline:\n",
        "\n",
        "‚úÖ Loaded custom dataset  \n",
        "‚úÖ Trained BPE tokenizer  \n",
        "‚úÖ Trained Transformer model  \n",
        "‚úÖ Exported to .bullet format  \n",
        "‚úÖ Tested and validated  \n",
        "‚úÖ Downloaded deployment package  \n",
        "\n",
        "### üìö Next Steps:\n",
        "\n",
        "- **Train Longer**: Increase `max_steps` to 2000+\n",
        "- **More Data**: Add 100+ training examples\n",
        "- **Bigger Model**: Increase `hidden_size` to 256\n",
        "- **Deploy**: Use the [User Manual](https://github.com/iShrikantBhosale/bullet-core/blob/master/BULLET_USER_MANUAL.md)\n",
        "\n",
        "### üîó Resources:\n",
        "\n",
        "üìò [User Manual](https://github.com/iShrikantBhosale/bullet-core/blob/master/BULLET_USER_MANUAL.md)  \n",
        "üìñ [Education Manual](https://github.com/iShrikantBhosale/bullet-core/blob/master/BULLET_EDUCATION_MANUAL.md)  \n",
        "üíª [GitHub](https://github.com/iShrikantBhosale/bullet-core)  \n",
        "üåê [Website](https://ishrikantbhosale.github.io/bullet-core/)  \n",
        "\n",
        "---\n",
        "\n",
        "**Created by Shrikant Bhosale** | Mentored by [Hintson.com](https://hintson.com)  \n",
        "üáÆüá≥ Made in India | Democratizing AI  \n",
        "¬© 2025 Bullet OS | MIT License"
      ]
    }
  ]
}